---
title: "Calculation of convergence points of correlation types"
author: "Joaquim Aguirre-Plans"
date: "6/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description

Calculate the sample size where a specific type of correlations converge. We will use two methods:
- Using the statistical formula
- Manually

```{r}
library(data.table)
library(dplyr)
library(igraph)
library(ggplot2)
require(ggrepel)
require(magrittr)
library(tidyr)
set.seed(1510)
options(bitmapType='cairo')
`%ni%` <- Negate(`%in%`)
```

## Statistical formula

Read the number of edges of each dataset:

```{r}
input_dir = '/home/j.aguirreplans/Projects/Scipher/SampleSize/scripts/SampleSizeShiny/data'
plots_dir = '/home/j.aguirreplans/Projects/Scipher/SampleSize/data/out/plots'
numbers_file = paste(input_dir, 'dataset_numbers_complete_graph.txt', sep='/')
numbers_df = fread(numbers_file)
numbers_df$dataset = tolower(numbers_df$dataset)
print(numbers_df)
```

Define function to calculate sample size for correlation:

```{r}
calculate_sample_size_for_pearson_correlation_using_z_distribution = function(r, total_num_edges, alpha=0.05, power=0.8){
  alpha_corrected = alpha / total_num_edges # Correct alpha by multiple error
  Za = qnorm(alpha, lower.tail=F)
  Za_corrected = qnorm(alpha_corrected, lower.tail=F)
  Zb = qnorm(power, lower.tail=F)
  N = ((Za + Zb) / (0.5 * log((1+r)/(1-r))))**2 + 3
  N_corrected = ((Za_corrected + Zb) / (0.5 * log((1+r)/(1-r))))**2 + 3
  return(list(N=N, N_corrected=N_corrected))
}

calculate_sample_size_for_pearson_correlation_using_t_distribution = function(r, total_num_edges, alpha=0.05){
  alpha_corrected = alpha / total_num_edges # Correct alpha by multiple error
  N = ( qt(alpha, df=1, lower.tail = F) * sqrt((1-r**2)) / r )**2 + 2
  N_corrected = ( qt(alpha_corrected, df=1, lower.tail = F) * sqrt((1-r**2)) / r )**2 + 2
  return(list(N=N, N_corrected=N_corrected))
}


calculate_sample_size_for_pearson_correlation_using_t_distribution_with_optimization = function(r, total_num_edges, alpha=0.05, N_guess=c(10000)){
  optimize_N = function(par){
    N = par[1]
    t_guess = qt(alpha, df=N-2, lower.tail = F)
    t = r * sqrt((N-2)) / sqrt((1-r**2))
    return((abs(t-t_guess)))
  }
  res = optim(par=N_guess, fn=optimize_N, method="Brent", lower=3, upper=50000)
  return(res$par)
}
```

Calculate sample size for different levels of correlation and different datasets:

```{r}
type_correlation_df = data.frame(type_correlation=c("weak", "moderate", "strong", "very strong"), lower_val=c(0.2,0.4,0.6,0.8), upper_val=c(0.4,0.6,0.8,NA))
types_correlation = c("weak", "moderate", "strong", "very strong")

cols = c("dataset", "type_correlation", "correlation", "sample_size_statistical", "sample_size_statistical_corrected")
sample_size_correlation_df = data.frame(matrix(ncol=length(cols),nrow=0, dimnames=list(NULL, cols)))
for (dataset in numbers_df$dataset){
  total_num_edges = (numbers_df %>% filter(dataset == !!dataset))$total_num_edges
  for (type_correlation in types_correlation){
    r = (type_correlation_df %>% filter(type_correlation == !!type_correlation))$lower_val
    #N_result = calculate_sample_size_for_pearson_correlation_using_z_distribution(r=r, total_num_edges=total_num_edges, alpha=0.05, power=0.5)
    #N_result = calculate_sample_size_for_pearson_correlation_using_t_distribution(r=r, total_num_edges=total_num_edges, alpha=0.05)
    N = calculate_sample_size_for_pearson_correlation_using_t_distribution_with_optimization(r=r, total_num_edges=total_num_edges, alpha=0.05, N_guess=c(10000))
    N_corrected = calculate_sample_size_for_pearson_correlation_using_t_distribution_with_optimization(r=r, total_num_edges=total_num_edges, alpha=0.05/total_num_edges, N_guess=c(10000))
    sample_size_correlation_df <- rbind(sample_size_correlation_df, data.frame(dataset=dataset, type_correlation=type_correlation, correlation=r, sample_size_statistical=N, sample_size_statistical_corrected=N_corrected))
  }
}

output_dir = '/home/j.aguirreplans/Projects/Scipher/SampleSize/scripts/SampleSizeShiny/data/example_pearson_pval_0.05'
theoretical_sample_size_file = paste(output_dir, 'theoretical_sample_size_for_correlations_of_datasets.txt', sep='/')
sample_size_correlation_df %>% fwrite(theoretical_sample_size_file)

print(sample_size_correlation_df)
```

## Manual

Read the number of edges for each dataset:

```{r}
input_dir = '/home/j.aguirreplans/Projects/Scipher/SampleSize/scripts/SampleSizeShiny/data'
topology_results_file = paste(input_dir, 'analysis_topology.csv', sep='/')
topology_results_df = fread(topology_results_file)
topology_results_df$dataset = paste(topology_results_df$dataset, topology_results_df$type_dataset, sep=":") # Join dataset and type_dataset
topology_results_df$dataset = tolower(topology_results_df$dataset)
head(topology_results_df)
```

Calculate sample size where each type of correlation converges.

```{r}
type_correlation_df = data.frame(type_correlation=c("very strong", "strong-very strong", "moderate-strong-very strong", "weak-moderate-strong-very strong"), lower_val=c(0.8,0.6,0.4,0.2), upper_val=c(NA,NA,NA,NA))
types_correlation = c("very strong", "strong-very strong", "moderate-strong-very strong", "weak-moderate-strong-very strong")
method = "pearson"
threshold = 0.05

cols = c("dataset", "type_correlation", "correlation", "sample_size_from_graphic", "num_edges_from_graphic", "sample_size_max_value")
size_convergence_correlation_df = data.frame(matrix(ncol=length(cols),nrow=0, dimnames=list(NULL, cols)))
for (dataset in numbers_df$dataset){
  # Calculate mean for all correlations
  topology_results_all_corr_df = topology_results_df %>% filter((method == !!method) & (dataset == !!dataset) & (type_correlation == "all") & (threshold == !!threshold)) %>% select(size, rep, num_edges)
topology_results_all_corr_by_size_df = topology_results_all_corr_df %>%
  group_by(size) %>%
  summarise(mean=mean(num_edges), sd=sd(num_edges)) %>%
  arrange(size)
  for (type_correlation in types_correlation){
    r = (type_correlation_df %>% filter(type_correlation == !!type_correlation))$lower_val
    topology_results_selected_df = topology_results_df %>% filter((method == !!method) & (dataset == !!dataset) & (type_correlation == !!type_correlation) & (threshold == !!threshold)) %>% select(size, rep, num_edges)
    topology_results_selected_by_size_df = topology_results_selected_df %>%
      group_by(size) %>%
      summarise(mean=mean(num_edges), sd=sd(num_edges)) %>%
      arrange(size)
    max_value = max(topology_results_selected_by_size_df$mean)
    sample_size_max_value = (topology_results_selected_by_size_df %>% filter(mean == max_value))$size
    
    # Compare mean of selected correlation to mean of all correlations and select first sample size with different number of edges
    sample_size_from_graphic = sample_size_prev = NA
    num_edges_from_graphic = num_edges_prev = NA
    for (i in 1:(length(topology_results_selected_by_size_df$size))){
      sample_size_selected = topology_results_selected_by_size_df$size[i]
      value_size_selected = (topology_results_selected_by_size_df %>% filter(size == sample_size_selected))$mean
      value_size_all_corr = (topology_results_all_corr_by_size_df %>% filter(size == sample_size_selected))$mean
      diff = abs(value_size_selected - value_size_all_corr)
      #print(c(dataset, type_correlation, sample_size_selected, value_size_selected, value_size_all_corr, diff))
      if(diff > 0){
        sample_size_from_graphic = sample_size_prev
        num_edges_from_graphic = num_edges_prev
        break
      } else {
        sample_size_prev = sample_size_selected
        num_edges_prev = value_size_selected
      }
    }

    size_convergence_correlation_df <- rbind(size_convergence_correlation_df, data.frame(dataset=dataset, type_correlation=type_correlation, correlation=r, sample_size_from_graphic=sample_size_from_graphic, num_edges_from_graphic=num_edges_from_graphic, sample_size_max_value=sample_size_max_value))
  }
}

print(size_convergence_correlation_df)
```

```{r}
topology_results_selected_df = topology_results_df %>% filter((method == "pearson") & (dataset == "tcga:tcga") & (threshold == 0.05) & (type_correlation %in% c("all", "very strong", "strong-very strong", "moderate-strong-very strong", "weak-moderate-strong-very strong"))) %>% select(size, rep, num_edges, type_correlation)
topology_results_selected_by_size_df = topology_results_selected_df %>%
  group_by(size, type_correlation) %>%
  summarise(mean=mean(num_edges), sd=sd(num_edges)) %>%
  arrange(size)

topology_results_plot = ggplot(topology_results_selected_df, aes(x=size, y=num_edges, col=type_correlation)) + 
  guides(col=guide_legend(title="Type of correlation")) +
  geom_point(alpha=0.5, size=3) +
  geom_line(data = topology_results_selected_by_size_df,
            aes(x = size, y = mean),
            lwd=1)

# Customize axes/labels/grid
topology_results_plot = topology_results_plot + 
  theme_linedraw() +
  xlab("Number of samples") +
  ylab("Number of significant edges") +
  theme(plot.title =  element_text(size = 17, face="bold"), axis.title = element_text(size = 16, face="bold"), axis.text = element_text(size = 15), legend.text = element_text(size = 14), legend.title=element_text(size=15, face="bold"), legend.position="bottom")

topology_results_plot

```

Compare statistical results with manual results

```{r}
convergence_results_df = sample_size_correlation_df %>% inner_join((size_convergence_correlation_df %>% select(-type_correlation)), by=c("dataset", "correlation")) %>% select(-sample_size_statistical, -sample_size_max_value)
convergence_results_df$sample_size_error = abs(convergence_results_df$sample_size_statistical_corrected - convergence_results_df$sample_size_from_graphic)
convergence_results_df %<>% separate(dataset, c("dataset_name", "type_dataset"), sep=":", remove=FALSE) 
convergence_results_df
```

```{r}
ggplot(convergence_results_df, aes(x=sample_size_error, y=correlation, col=dataset_name)) + 
  geom_point() +
  theme_linedraw() +
  theme(plot.title =  element_text(size = 17, face="bold"), axis.title = element_text(size = 16, face="bold"), axis.text = element_text(size = 15), legend.text = element_text(size = 14), legend.title=element_text(size=15, face="bold")) +
  xlab("Sample size error") +
  ylab("Correlation threshold") +
  guides(col=guide_legend(title="Type of dataset")) +
  geom_text_repel(
    data = subset(convergence_results_df, dataset %in% c("tcga:tcga")),
    aes(label = dataset),
    size = 5,
    box.padding = unit(0.35, "lines"),
    point.padding = unit(0.3, "lines")
  )
plot_file = paste(plots_dir, "error_predicting_correlation_convergence_points.png", sep="/")
ggsave(
  plot_file,
  dpi = 1200,
  width = 10000,
  height = 6000,
  units = c("px")
)
```

## Relationship between stretched exponential model parameters and saturation points

We calculate the stretched exponential model for each dataset:

```{r}
#'  calculate_stretched_exponential_model_by_optimization
#'  Formula to find linear fit between log(ln(L)-ln(s)) and log(N) that has the minimum value of 1-R**2 using optimization.
#'  @param results_dataframe Dataframe containing the data.
#'  @param y_parameter Parameter of the Y axis.
#'  @param x_parameter Parameter of the X axis.
#'  @param L_guess Initial values for the parameters to be optimized over.
#'  
calculate_stretched_exponential_model_by_optimization = function(results_dataframe, y_parameter, x_parameter, L_guess=c(2)){
  # Calculate mean of repetitions from same sample size
  results_dataframe_mean = results_dataframe %>% 
    arrange(get(x_parameter), rep) %>%
    group_by(get(x_parameter)) %>%
    summarise_at(vars(all_of(y_parameter)), list(mean=mean, median=median, sd=sd)) %>%
    rename(!!x_parameter := "get(x_parameter)") %>%
    ungroup()
  # Define s and N
  s = results_dataframe_mean$mean / max(results_dataframe_mean$mean)
  N = results_dataframe_mean$size
  # Define function to get linear fit
  linear_fit = function(data, par){
    # Calculate ln(L)-ln(s)
    L = par[1]
    s_rec = log(L) - log(data$s)
    # Calculate linear regression between ln(L)-ln(s) and ln(N)
    lm_result = summary(lm(log(s_rec)~log(data$N)))
    # The function returns the result of 1-R**2
    return(1-lm_result$adj.r.squared)
  }
  # Get minimization of error in linear fit
  #res = optim(par=L_guess, fn=linear_fit, data=data.frame(s=s, N=N), method="Nelder-Mead")
  res = optim(par=L_guess, fn=linear_fit, data=data.frame(s=s, N=N), method="Brent", lower=0, upper=5)
  L=res$par
  s_rec=log(L)-log(s)
  # This is the linear fit containing L
  lm_summary = summary(lm(log(s_rec)~log(N)))
  # This is the data used to obtain the final model
  used_data = data.frame(y=log(s_rec), x=log(N))
  return(list(lm_summary=lm_summary, used_data=used_data, a=coef(lm_summary)[2], b=coef(lm_summary)[1], L=L, adj.r.squared=lm_summary$adj.r.squared))
}

#'  calculate_predictions_using_stretched_exponential_model_optimized
#'  Formula to calculate exponential decay of significant interactions from a list of sample sizes.
#'  This formula requires the use of the L parameter
#'  @param x List of sample sizes.
#'  @param a Slope coefficient.
#'  @param b Intercept coefficient.
#'  
calculate_predictions_using_stretched_exponential_model_optimized = function(x, L, a, b){
  y = exp(log(L) - exp(b+a*log(x)))
  return(y)
}

#'  calculate_analytical_model
#'  Function to calculate the analytical model from different options
#'  @param results_dataframe Dataframe containing the data.
#'  @param y_parameter Parameter of the Y axis.
#'  @param x_parameter Parameter of the X axis.
#'  @param model Name of the model used.
#'  
calculate_analytical_model = function(results_dataframe, y_parameter, x_parameter, model){
  if(model == "Logarithmic"){
    model_output = get_logarithmic_tendency(results_dataframe=results_dataframe, y_parameter=y_parameter, x_parameter=x_parameter)
    model_result = (log(sort(unique(results_dataframe[[x_parameter]])))*model_output$a + model_output$b)
  } else if(model == "Stretched exponential (by optimization)"){
    model_output = calculate_stretched_exponential_model_by_optimization(results_dataframe=results_dataframe, y_parameter=y_parameter, x_parameter=x_parameter, L_guess=c(2))
    model_result = calculate_predictions_using_stretched_exponential_model_optimized(x=sort(unique(results_dataframe[[x_parameter]])), L=model_output$L, a=model_output$a, b=model_output$b)
  } else if(model == "Stretched exponential (by linear fit)"){
    model_output = calculate_stretched_exponential_model_by_linear_fit(results_dataframe=results_dataframe, y_parameter=y_parameter, x_parameter=x_parameter)
    model_result = calculate_predictions_using_stretched_exponential_model_from_linear_fit(x=sort(unique(results_dataframe[[x_parameter]])), a=model_output$a, b=model_output$b)
  }
  return(list(model_output=model_output, model_result=model_result))
}

#'  calculate_prediction_from_analytical_model
#'  Function to calculate predictions using a specific analytical model
#'  @param model Name of the model used.
#'  @param x_list List of values in the x axis (e.g. size)
#'  @param a Parameter a.
#'  @param b Parameter b.
#'  @param L Parameter L.
#'  
calculate_prediction_from_analytical_model = function(model, x_list, a, b, L){
  if(model == "Logarithmic"){
    prediction_result = (log(x_list)*a + b)
  } else if(model == "Stretched exponential (by optimization)"){
    prediction_result = calculate_predictions_using_stretched_exponential_model_optimized(x=x_list, L=L, a=a, b=b)
  } else if(model == "Stretched exponential (by linear fit)"){
    prediction_result = calculate_predictions_using_stretched_exponential_model_from_linear_fit(x=x_list, a=a, b=b)
  }
  return(prediction_result)
}
```

```{r}
cols = c("dataset", "a", "b", "L", "max_value_in_dataset")
analytical_model_parameters_df = data.frame(matrix(ncol=length(cols),nrow=0, dimnames=list(NULL, cols)))
for (dataset in unique(convergence_results_df$dataset)){
  # Select results
  results_selected_df = topology_results_df %>% filter((method == !!method) & (dataset == !!dataset) & (type_correlation == !!type_correlation) & (threshold == !!threshold)) %>% select(size, rep, num_edges)
  # Calculate the analytical model
  output_variable = calculate_analytical_model(results_dataframe=results_selected_df, y_parameter="num_edges", x_parameter="size", model="Stretched exponential (by optimization)")
  # Calculate prediction
  prediction_result = calculate_prediction_from_analytical_model(model="Stretched exponential (by optimization)", x_list=seq(10, 50000, 10), a=output_variable$model_output$a, b=output_variable$model_output$b, L=output_variable$model_output$L)
  # Un-normalize y axis of analytical curve (if necessary), because the analytical curve of Power law is normalized by default
  max_value_in_dataset = max(results_selected_df)
  output_variable$model_result = output_variable$model_result * max_value_in_dataset
  prediction_result = prediction_result * max_value_in_dataset
  # Save parameters of the analytical model
  #print(c(dataset, output_variable$model_output$a, output_variable$model_output$b, output_variable$model_output$L))
  analytical_model_parameters_df <- rbind(analytical_model_parameters_df, data.frame(dataset=dataset, a=output_variable$model_output$a, b=output_variable$model_output$b, L=output_variable$model_output$L, max_value_in_dataset=max_value_in_dataset))
}

print(analytical_model_parameters_df)
```

Explore the relationship between stretched exponential parameters and correlation critical values:

```{r}
convergence_results_long_df = convergence_results_df %>% select(-type_correlation,-dataset_name, -type_dataset) %>% pivot_longer(c(-dataset, -correlation), names_to = "correlation_parameter_name", values_to = "correlation_parameter_value") %>% unite(correlation_parameter_name, correlation_parameter_name:correlation)
analytical_model_parameters_long_df = analytical_model_parameters_df %>% pivot_longer(-dataset, names_to="model_parameter_name", values_to="model_parameter_value")
cols = c("correlation_parameter", "model_parameter", "cor", "pval")
results_correlation_df = data.frame(matrix(ncol=length(cols),nrow=0, dimnames=list(NULL, cols)))
for (correlation_parameter in unique(convergence_results_long_df$correlation_parameter_name)){
  for (model_parameter in unique(analytical_model_parameters_long_df$model_parameter_name)){
    values_convergence = (convergence_results_long_df %>% filter(correlation_parameter_name == correlation_parameter))$correlation_parameter_value
    values_analytical = (analytical_model_parameters_long_df %>% filter(model_parameter_name == model_parameter))$model_parameter_value
    if (sum(!is.na(values_convergence)) > 2){
      cor_result = cor.test(values_convergence, values_analytical, method = "pearson")
      cor_value = as.numeric(cor_result$estimate)
      cor_pvalue = cor_result$p.value
    } else {
      cor_value = NA
      cor_pvalue = NA
    }
    results_correlation_df <- rbind(results_correlation_df, data.frame(correlation_parameter=correlation_parameter, model_parameter=model_parameter, cor=cor_value, pval=cor_pvalue))
  }
}
```

```{r}
results_correlation_df %>% filter((correlation_parameter %in% c("sample_size_from_graphic_0.4", "sample_size_from_graphic_0.6", "sample_size_from_graphic_0.8"))) %>% ggplot(aes(x=cor, fill=model_parameter, color=model_parameter)) +
  geom_histogram(position="identity", binwidth = 0.1, alpha=0.2)
```

```{r}
results_correlation_df %>% filter((correlation_parameter %in% c("sample_size_statistical_corrected_0.2", "sample_size_statistical_corrected_0.4", "sample_size_statistical_corrected_0.6", "sample_size_statistical_corrected_0.8"))) %>% ggplot(aes(x=cor, fill=model_parameter, color=model_parameter)) +
  geom_histogram(position="identity", binwidth = 0.1, alpha=0.2)
```

```{r}
analytical_model_parameters_df %<>% separate(dataset, c("dataset_name", "type_dataset"), sep=":", remove=FALSE) 
analytical_model_parameters_df %>% ggplot(aes(x=a, y=L)) + 
  geom_point(aes(col=dataset_name)) +
  #geom_smooth(method='lm', col="black", size=0.5) +
  theme_linedraw() +
  theme(plot.title =  element_text(size = 17, face="bold"), axis.title = element_text(size = 16, face="bold"), axis.text = element_text(size = 15), legend.text = element_text(size = 14), legend.title=element_text(size=15, face="bold")) +
  guides(col=guide_legend(title="Type of dataset")) +
  geom_text_repel(
    data = subset(analytical_model_parameters_df, dataset %in% c("tcga:tcga", "tcga:tcga-lusc", "tcga:tcga-coad", "tcga:tcga-brca", "gtex:whole.blood", "gtex:artery.tibial", "scipher:complete.dataset")),
    aes(label = dataset, col = dataset_name),
    size = 5,
    box.padding = unit(0.35, "lines"),
    point.padding = unit(0.3, "lines")
  )
plot_file = paste(plots_dir, "a_vs_L.png", sep="/")
ggsave(
  plot_file,
  dpi = 1200,
  width = 10000,
  height = 6000,
  units = c("px")
)
```

We observe a clear trend between parameters a and L:the less negative a is, the higher L is. We see that there are some outliers (TCGA, TCGA-BRCA) that do not follow this trend, needing a higher number of samples to converge to a given % of edges than the rest of the datasets. This reflcts that the outliers of this plot are datasets with less quality, or more heterogeneous.


```{r}
analytical_model_convergence_parameters_df = analytical_model_parameters_df %>% inner_join(convergence_results_df, by=c("dataset", "dataset_name", "type_dataset")) %>% inner_join(numbers_df, by="dataset")
analytical_model_convergence_parameters_df$num_edges_from_statistical_corrected = calculate_prediction_from_analytical_model(model="Stretched exponential (by optimization)", x_list=analytical_model_convergence_parameters_df$sample_size_statistical_corrected, a=analytical_model_convergence_parameters_df$a, b=analytical_model_convergence_parameters_df$b, L=analytical_model_convergence_parameters_df$L)
analytical_model_convergence_parameters_df$num_edges_from_statistical_corrected = analytical_model_convergence_parameters_df$num_edges_from_statistical_corrected * analytical_model_convergence_parameters_df$max_value_in_dataset
analytical_model_convergence_parameters_df$num_edges_from_graphic_norm = analytical_model_convergence_parameters_df$num_edges_from_graphic / analytical_model_convergence_parameters_df$total_num_edges
analytical_model_convergence_parameters_df$num_edges_from_statistical_corrected_norm = analytical_model_convergence_parameters_df$num_edges_from_statistical_corrected / analytical_model_convergence_parameters_df$total_num_edges
analytical_model_convergence_parameters_df$correlation = as.numeric(analytical_model_convergence_parameters_df$correlation)
analytical_model_convergence_parameters_df %>% ggplot(aes(x=a, y=num_edges_from_statistical_corrected_norm, col=correlation)) + 
  geom_point() +
  theme_linedraw() +
  theme(plot.title =  element_text(size = 17, face="bold"), axis.title = element_text(size = 16, face="bold"), axis.text = element_text(size = 15), legend.text = element_text(size = 14), legend.title=element_text(size=15, face="bold")) +
  guides(col=guide_legend(title="Type of dataset")) +
  geom_text_repel(
    data = subset(analytical_model_convergence_parameters_df, dataset %in% c("tcga:tcga", "tcga:tcga-lusc", "tcga:tcga-coad", "tcga:tcga-brca", "gtex:whole.blood", "gtex:artery.tibial", "scipher:complete.dataset")),
    aes(label = dataset),
    size = 5,
    box.padding = unit(0.35, "lines"),
    point.padding = unit(0.3, "lines")
  )

for (type_correlation_selected in unique(analytical_model_convergence_parameters_df$type_correlation)){
  print(type_correlation_selected)
  corr_specific_plot = analytical_model_convergence_parameters_df %>% filter(type_correlation == type_correlation_selected)  %>% filter(!(dataset == "tcga:tcga")) %>% 
    ggplot(aes(x=a, y=num_edges_from_statistical_corrected_norm, col=dataset_name)) + 
    geom_point() +
    theme_linedraw() +
    theme(plot.title =  element_text(size = 17, face="bold"), axis.title = element_text(size = 16, face="bold"), axis.text = element_text(size = 15), legend.text = element_text(size = 14), legend.title=element_text(size=15, face="bold")) +
    xlab("a") +
    ylab("Num. of correlations / total at convergence") +
    guides(col=guide_legend(title="Type of dataset")) +
    geom_text_repel(
      data = subset((analytical_model_convergence_parameters_df %>% filter(type_correlation == type_correlation_selected)), dataset %in% c("tcga:tcga-lusc", "tcga:tcga-coad", "tcga:tcga-brca", "gtex:whole.blood", "gtex:artery.tibial", "scipher:complete.dataset")),
      aes(label = dataset),
      size = 5,
      box.padding = unit(0.35, "lines"),
      point.padding = unit(0.3, "lines")
    )
  print(corr_specific_plot)
  plot_file = paste(plots_dir, paste("a_vs_percentage_correlation_at_convergence_", type_correlation_selected, ".png", sep=""), sep="/")
  ggsave(
    plot_file,
    dpi = 1200,
    width = 10000,
    height = 6000,
    units = c("px")
  )
}

```

## Relationship between stretched exponential model parameters and dataset parameters

```{r}
#'  calculate_mean_variance_across_genes
#'  Function to calculate the mean variance across expression of genes
#'  @param rnaseq_df Dataframe containing the data.
#'  
calculate_mean_variance_across_genes = function(rnaseq_df){
  return(mean(apply(rnaseq_df[,-1], 1, sd)))
}
```

For GTEx:

```{r}
gtex_rnaseq_dir = "/home/j.aguirreplans/Databases/GTEx/v8/tpm_filtered_files_by_tissue"
gtex_samples_subjects_file = '/home/j.aguirreplans/Databases/GTEx/v8/GTEx_Annotations_SamplesSubjectsMerged.txt'
gtex_samples_df = fread(gtex_samples_subjects_file)
topology_results_df = topology_results_df %>% separate(dataset, c("dataset_name", NA), sep=":", remove=FALSE)
numbers_df %<>% separate(dataset, c("dataset_name", "type_dataset"), sep=":", remove=FALSE)
gtex_datasets = unique((topology_results_df %>% filter(dataset_name == "gtex"))$dataset)
cols = c("dataset", "tissue", "num_genes", "mean_iqr_genes", "median_iqr_genes", "num_male", "num_female", "num_age_20_29", "num_age_30_39", "num_age_40_49", "num_age_50_59", "num_age_60_69", "num_age_70_79")
gtex_parameters_df = data.frame(matrix(ncol=length(cols),nrow=0, dimnames=list(NULL, cols)))
for (gtex_dataset in gtex_datasets){
  tissue = unique((topology_results_df %>% filter(dataset == gtex_dataset))$type_dataset)
  gtex_samples_tissue_df = gtex_samples_df %>% filter(SMTSD.no.sp.char == tissue)
  num_genes = (numbers_df %>% filter(dataset == gtex_dataset))$total_num_nodes
  num_male = length(unique((gtex_samples_tissue_df %>% filter(SEX == 1))$SUBJID))
  num_female = length(unique((gtex_samples_tissue_df %>% filter(SEX == 2))$SUBJID))
  num_age_20_29 = length(unique((gtex_samples_tissue_df %>% filter(AGE == "20-29"))$SUBJID))
  num_age_30_39 = length(unique((gtex_samples_tissue_df %>% filter(AGE == "30-39"))$SUBJID))
  num_age_40_49 = length(unique((gtex_samples_tissue_df %>% filter(AGE == "40-49"))$SUBJID))
  num_age_50_59 = length(unique((gtex_samples_tissue_df %>% filter(AGE == "50-59"))$SUBJID))
  num_age_60_69 = length(unique((gtex_samples_tissue_df %>% filter(AGE == "60-69"))$SUBJID))
  num_age_70_79 = length(unique((gtex_samples_tissue_df %>% filter(AGE == "70-79"))$SUBJID))
  gtex_rnaseq_file = paste(gtex_rnaseq_dir, paste("gtex_rnaseq_", tissue, ".gct", sep=""), sep="/")
  gtex_rnaseq_df = fread(gtex_rnaseq_file)
  iqr_across_genes = apply(gtex_rnaseq_df[,-1], 1, IQR)
  mean_iqr_genes = mean(iqr_across_genes)
  median_iqr_genes = median(iqr_across_genes)
  gtex_parameters_df = rbind(gtex_parameters_df, data.frame(dataset=gtex_dataset, tissue=tissue, num_genes=num_genes, mean_iqr_genes=mean_iqr_genes, median_iqr_genes=median_iqr_genes, num_male=num_male, num_female=num_female, num_age_20_29=num_age_20_29, num_age_30_39=num_age_30_39, num_age_40_49=num_age_40_49, num_age_50_59=num_age_50_59, num_age_60_69=num_age_60_69, num_age_70_79=num_age_70_79))
}
gtex_parameters_df$fraction_male = gtex_parameters_df$num_male / (gtex_parameters_df$num_male + gtex_parameters_df$num_female)
gtex_parameters_df$fraction_female = gtex_parameters_df$num_female / (gtex_parameters_df$num_male + gtex_parameters_df$num_female)
gtex_parameters_df$fraction_age_20_29 = gtex_parameters_df$num_age_20_29 / (gtex_parameters_df$num_age_20_29 + gtex_parameters_df$num_age_30_39 + gtex_parameters_df$num_age_40_49 + gtex_parameters_df$num_age_50_59 + gtex_parameters_df$num_age_60_69 + gtex_parameters_df$num_age_70_79)
gtex_parameters_df$fraction_age_30_39 = gtex_parameters_df$num_age_30_39 / (gtex_parameters_df$num_age_20_29 + gtex_parameters_df$num_age_30_39 + gtex_parameters_df$num_age_40_49 + gtex_parameters_df$num_age_50_59 + gtex_parameters_df$num_age_60_69 + gtex_parameters_df$num_age_70_79)
gtex_parameters_df$fraction_age_40_49 = gtex_parameters_df$num_age_40_49 / (gtex_parameters_df$num_age_20_29 + gtex_parameters_df$num_age_30_39 + gtex_parameters_df$num_age_40_49 + gtex_parameters_df$num_age_50_59 + gtex_parameters_df$num_age_60_69 + gtex_parameters_df$num_age_70_79)
gtex_parameters_df$fraction_age_50_59 = gtex_parameters_df$num_age_50_59 / (gtex_parameters_df$num_age_20_29 + gtex_parameters_df$num_age_30_39 + gtex_parameters_df$num_age_40_49 + gtex_parameters_df$num_age_50_59 + gtex_parameters_df$num_age_60_69 + gtex_parameters_df$num_age_70_79)
gtex_parameters_df$fraction_age_60_69 = gtex_parameters_df$num_age_60_69 / (gtex_parameters_df$num_age_20_29 + gtex_parameters_df$num_age_30_39 + gtex_parameters_df$num_age_40_49 + gtex_parameters_df$num_age_50_59 + gtex_parameters_df$num_age_60_69 + gtex_parameters_df$num_age_70_79)
gtex_parameters_df$fraction_age_70_79 = gtex_parameters_df$num_age_70_79 / (gtex_parameters_df$num_age_20_29 + gtex_parameters_df$num_age_30_39 + gtex_parameters_df$num_age_40_49 + gtex_parameters_df$num_age_50_59 + gtex_parameters_df$num_age_60_69 + gtex_parameters_df$num_age_70_79)
gtex_parameters_df
```

```{r}
gtex_parameters_long_df = gtex_parameters_df %>% inner_join((analytical_model_parameters_df %>% select(-type_dataset, -dataset_name, -max_value_in_dataset)), by=c("dataset")) %>% pivot_longer(c(-dataset, -tissue, -a, -b, -L), names_to = "dataset_parameter_name", values_to = "dataset_parameter_value")
gtex_dataset_parameters = c("num_genes", "mean_iqr_genes", "median_iqr_genes", "fraction_male", "fraction_female", "fraction_age_20_29", "fraction_age_30_39", "fraction_age_40_49", "fraction_age_50_59", "fraction_age_60_69", "fraction_age_70_79")
for (parameter in gtex_dataset_parameters){
  a_vs_parameter_plot = gtex_parameters_long_df %>% filter(dataset_parameter_name == parameter) %>%
    ggplot(aes(x=a, y=dataset_parameter_value, col=tissue)) + 
    geom_point() +
    theme_linedraw() +
    theme(plot.title =  element_text(size = 17, face="bold"), axis.title = element_text(size = 16, face="bold"), axis.text = element_text(size = 15), legend.text = element_text(size = 14), legend.title=element_text(size=15, face="bold")) +
    xlab("a") +
    ylab(parameter) +
    guides(col=guide_legend(title="Tissue")) +
    geom_text_repel(
      data = subset((gtex_parameters_long_df %>% filter(dataset_parameter_name == parameter)), dataset %in% c("gtex:whole.blood", "gtex:artery.tibial")),
      aes(label = tissue),
      size = 5,
      box.padding = unit(0.35, "lines"),
      point.padding = unit(0.3, "lines")
    )
  print(a_vs_parameter_plot)
  plot_file = paste(plots_dir, paste("a_vs_dataset_parameter_", parameter, ".png", sep=""), sep="/")
  ggsave(
    plot_file,
    dpi = 1200,
    width = 10000,
    height = 6000,
    units = c("px")
  )
}

```

```{r}
gtex_rnaseq_wh_file = paste(gtex_rnaseq_dir, "gtex_rnaseq_Whole.Blood.gct", sep="/")
gtex_rnaseq_wh_df = fread(gtex_rnaseq_wh_file)
gtex_rnaseq_at_file = paste(gtex_rnaseq_dir, "gtex_rnaseq_Artery.Tibial.gct", sep="/")
gtex_rnaseq_at_df = fread(gtex_rnaseq_at_file)
sd_across_genes = apply(gtex_rnaseq_df[,-1], 1, IQR)


```

